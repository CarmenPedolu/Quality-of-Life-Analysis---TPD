{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6y0fNGcmnFII",
        "outputId": "3552e3da-7af3-4778-94a9-9eeaf8d0e2c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Starting script execution...\n",
            "WARNING:root:Spark session initialized.\n",
            "WARNING:root:Downloading and loading dataset with kagglehub...\n",
            "WARNING:root:Loading CSV into pandas dataframe...\n",
            "WARNING:root:Loaded 31430 rows.\n",
            "WARNING:root:Loaded geocodes cache with 3143 entries.\n",
            "WARNING:root:Starting geocoding for all rows...\n",
            "WARNING:root:Geocoding completed.\n",
            "WARNING:root:Converted pandas dataframe to Spark dataframe.\n",
            "WARNING:root:Calculating total_cost per person...\n",
            "WARNING:root:Aggregating average total_cost per state and county...\n",
            "WARNING:root:Calculating min and max total_cost...\n",
            "WARNING:root:Min total_cost: 17807.963076, Max total_cost: 52908.273440000004\n",
            "WARNING:root:Calculating ranking column...\n",
            "WARNING:root:Ranking column calculated.\n",
            "WARNING:root:Selecting and saving final results to CSV...\n",
            "WARNING:root:Script finished successfully and CSV saved.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import folium\n",
        "from folium.plugins import MarkerCluster\n",
        "from ipywidgets import interact, IntSlider, HTML\n",
        "import os\n",
        "from geopy.geocoders import Nominatim\n",
        "import kagglehub\n",
        "import re\n",
        "import numpy as np\n",
        "import requests\n",
        "import logging\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, concat_ws, udf, mean, when, lit, max as spark_max, min as spark_min\n",
        "from pyspark.sql.types import StringType, IntegerType, StructType, StructField\n",
        "from pyspark.sql.types import DoubleType, ArrayType\n",
        "from pyspark.sql.functions import regexp_extract\n",
        "\n",
        "\n",
        "# Setup basic logging to console with timestamps\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')\n",
        "\n",
        "logging.warning(\"Starting script execution...\")\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"US_Cost_of_living\").getOrCreate()\n",
        "logging.warning(\"Spark session initialized.\")\n",
        "\n",
        "# Load dataset\n",
        "logging.warning(\"Downloading and loading dataset with kagglehub...\")\n",
        "path = kagglehub.dataset_download(\"asaniczka/us-cost-of-living-dataset-3171-counties\")\n",
        "csv_file = os.path.join(path, \"cost_of_living_us.csv\")\n",
        "\n",
        "# Load CSV into pandas first\n",
        "logging.warning(\"Loading CSV into pandas dataframe...\")\n",
        "df = pd.read_csv(csv_file, encoding=\"latin1\")\n",
        "logging.warning(f\"Loaded {len(df)} rows.\")\n",
        "\n",
        "# Create a geolocator to convert county names to coordinates\n",
        "geolocator = Nominatim(user_agent=\"col_map_app\", timeout=10)\n",
        "\n",
        "# Define geocode cache file\n",
        "geocodes_cache_file = \"geocodes_cache_col.csv\"\n",
        "\n",
        "# Load existing cached geocodes, if any\n",
        "try:\n",
        "    geocodes_cache = pd.read_csv(geocodes_cache_file)\n",
        "    logging.warning(f\"Loaded geocodes cache with {len(geocodes_cache)} entries.\")\n",
        "except FileNotFoundError:\n",
        "    geocodes_cache = pd.DataFrame(columns=[\"state\", \"county\", \"latitude\", \"longitude\"])\n",
        "    logging.warning(\"No existing geocodes cache found, starting fresh.\")\n",
        "\n",
        "# Function to get coordinates for a given county and state with logging\n",
        "def get_coordinates(state, county):\n",
        "    global geocodes_cache\n",
        "\n",
        "    logging.info(f\"Geocoding: {county}, {state}\")\n",
        "\n",
        "    # Check if the coordinates are already in the cache\n",
        "    cached_location = geocodes_cache[(geocodes_cache['state'] == state) &\n",
        "                                     (geocodes_cache['county'] == county)]\n",
        "\n",
        "    if not cached_location.empty:\n",
        "        lat = cached_location['latitude'].iloc[0]\n",
        "        lon = cached_location['longitude'].iloc[0]\n",
        "\n",
        "        # If coordinates are NaN or None, skip geocoding (known missing location)\n",
        "        if pd.isna(lat) or pd.isna(lon):\n",
        "            logging.info(f\"Previously marked as not found: {county}, {state}, skipping geocoding.\")\n",
        "            return None\n",
        "        else:\n",
        "            logging.info(f\"Cache hit for {county}, {state}\")\n",
        "            return (lat, lon)\n",
        "\n",
        "    try:\n",
        "        county_name = county.replace(\" County\", \"\")\n",
        "        location = geolocator.geocode({\"city\": county_name, \"state\": state, \"country\": \"USA\"})\n",
        "\n",
        "        if location:\n",
        "            logging.info(f\"Geocoded {county}, {state} to ({location.latitude}, {location.longitude})\")\n",
        "            new_location = pd.DataFrame({\"state\": [state], \"county\": [county],\n",
        "                                         \"latitude\": [location.latitude], \"longitude\": [location.longitude]})\n",
        "        else:\n",
        "            logging.warning(f\"Geocode not found for {county}, {state}. Marking as missing.\")\n",
        "            # Mark missing in cache with NaN coordinates\n",
        "            new_location = pd.DataFrame({\"state\": [state], \"county\": [county],\n",
        "                                         \"latitude\": [np.nan], \"longitude\": [np.nan]})\n",
        "\n",
        "        geocodes_cache = pd.concat([geocodes_cache, new_location], ignore_index=True)\n",
        "        geocodes_cache.to_csv(geocodes_cache_file, index=False)\n",
        "\n",
        "        if location:\n",
        "            return (location.latitude, location.longitude)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Geocoding location error for {county}, {state}: {e}\")\n",
        "        return None\n",
        "\n",
        "logging.warning(\"Starting geocoding for all rows...\")\n",
        "# Apply your get_coordinates function using pandas apply with caching\n",
        "df['coordinates'] = df.apply(lambda row: get_coordinates(row['state'], row['county']), axis=1)\n",
        "logging.warning(\"Geocoding completed.\")\n",
        "\n",
        "# Extract latitude and longitude into separate columns before creating Spark DataFrame\n",
        "df['latitude'] = df['coordinates'].apply(lambda x: x[0] if x else None)\n",
        "df['longitude'] = df['coordinates'].apply(lambda x: x[1] if x else None)\n",
        "\n",
        "# Drop the original 'coordinates' column as it's not needed in Spark DataFrame\n",
        "df = df.drop(columns=['coordinates'])\n",
        "\n",
        "\n",
        "# Then create spark dataframe from pandas df\n",
        "df_spark = spark.createDataFrame(df)\n",
        "logging.warning(\"Converted pandas dataframe to Spark dataframe.\")\n",
        "\n",
        "# Extract numbers using regex directly in Spark\n",
        "df_spark = df_spark.withColumn(\"parents\", regexp_extract(col(\"family_member_count\"), r\"(\\d+)\\s*p\", 1).cast(\"int\"))\n",
        "df_spark = df_spark.withColumn(\"children\", regexp_extract(col(\"family_member_count\"), r\"(\\d+)\\s*c\", 1).cast(\"int\"))\n",
        "\n",
        "# Fill missing values with 0 (for rows where regex didn't match)\n",
        "df_spark = df_spark.fillna({'parents': 0, 'children': 0})\n",
        "\n",
        "# Compute total family members\n",
        "df_spark = df_spark.withColumn(\"family_members\", col(\"parents\") + col(\"children\"))\n",
        "logging.warning(\"Calculating total_cost per person...\")\n",
        "# Calculate total_cost per person\n",
        "# Use null-safe division by checking for family_members > 0\n",
        "df_spark = df_spark.withColumn(\n",
        "    \"total_cost\",\n",
        "    col(\"total_cost\").cast(DoubleType()) /\n",
        "    when(col(\"family_members\") > 0, col(\"family_members\")).otherwise(None)\n",
        ")\n",
        "\n",
        "logging.warning(\"Aggregating average total_cost per state and county...\")\n",
        "# Average all total_cost per person from the same unique combination of county and state to an averaged result per combination\n",
        "df_spark = df_spark.groupBy(\"state\", \"county\", \"latitude\", \"longitude\").agg(mean(\"total_cost\").alias(\"total_cost\"))\n",
        "\n",
        "# Calculate min and max\n",
        "logging.warning(\"Calculating min and max total_cost...\")\n",
        "# Collect the min and max values safely, handling potential None results\n",
        "min_result = df_spark.agg(spark_min(\"total_cost\")).collect()\n",
        "min_val = min_result[0][0] if min_result and min_result[0][0] is not None else None\n",
        "\n",
        "max_result = df_spark.agg(spark_max(\"total_cost\")).collect()\n",
        "max_val = max_result[0][0] if max_result and max_result[0][0] is not None else None\n",
        "\n",
        "logging.warning(f\"Min total_cost: {min_val}, Max total_cost: {max_val}\")\n",
        "\n",
        "\n",
        "logging.warning(\"Calculating ranking column...\")\n",
        "# Add ranking column\n",
        "# Check if min_val and max_val are valid before calculating the ranking\n",
        "if min_val is not None and max_val is not None and (max_val - min_val) != 0:\n",
        "    df_spark = df_spark.withColumn(\"cost_of_living_ranking\", 1 - ((col(\"total_cost\") - lit(min_val)) / (lit(max_val - min_val))))\n",
        "    logging.warning(\"Ranking column calculated.\")\n",
        "else:\n",
        "    # If min/max are None or the range is zero, ranking cannot be calculated meaningfully\n",
        "    logging.warning(\"Could not calculate ranking column due to invalid min/max values or zero range. Setting ranking to None.\")\n",
        "    df_spark = df_spark.withColumn(\"cost_of_living_ranking\", lit(None).cast(DoubleType())) # Assign None to ranking column\n",
        "\n",
        "logging.warning(\"Selecting and saving final results to CSV...\")\n",
        "# Select desired columns and convert to Pandas\n",
        "# Ensure 'cost_of_living_ranking' column is included even if it's all None\n",
        "ranking_df = df_spark.select(\"state\", \"county\", \"latitude\", \"longitude\", \"total_cost\", \"cost_of_living_ranking\").toPandas()\n",
        "\n",
        "# Save to CSV\n",
        "ranking_df.to_csv(\"cost_of_living_ranking.csv\", index=False)\n",
        "\n",
        "logging.warning(\"Script finished successfully and CSV saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load the ranked crime dataset\n",
        "col_df = pd.read_csv(\"cost_of_living_ranking.csv\")\n",
        "\n",
        "# Step 3: Remove \"county\" or \"COUNTY\" from county column values and strip whitespace\n",
        "col_df[\"county\"] = col_df[\"county\"].str.replace(r\"\\bcounty\\b\", \"\", flags=re.IGNORECASE, regex=True).str.strip()\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "col_df.to_csv(\"Cost_of_living_ranked.csv\", index=False)\n",
        "\n",
        "print(\"Processed data saved to Cost_of_living_ranked.csv\")\n",
        "print(col_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytzH08L5W2QA",
        "outputId": "eff28f22-b8dd-4af4-900a-c805df058497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed data saved to Cost_of_living_ranked.csv\n",
            "  state                     county   latitude   longitude    total_cost  \\\n",
            "0    AK  Hoonah-Angoon Census Area        NaN         NaN  24729.205274   \n",
            "1    AR                    Johnson  36.132856  -94.165482  19146.670728   \n",
            "2    CO                 Las Animas  38.066456 -103.222478  21664.262724   \n",
            "3    CO                      Routt  38.134233  -85.466622  27986.883290   \n",
            "4    GA                     Fannin        NaN         NaN  20829.709668   \n",
            "\n",
            "   cost_of_living_ranking  \n",
            "0                0.802815  \n",
            "1                0.961861  \n",
            "2                0.890135  \n",
            "3                0.710005  \n",
            "4                0.913911  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from shapely.geometry import Point\n",
        "\n",
        "# Load cost of living ranked CSV (with latitude and longitude)\n",
        "col_df = pd.read_csv(\"Cost_of_living_ranked.csv\")\n",
        "\n",
        "# Create geometry column from latitude and longitude\n",
        "col_df[\"geometry\"] = col_df.apply(\n",
        "    lambda row: Point(row[\"longitude\"], row[\"latitude\"])\n",
        "                if pd.notna(row[\"latitude\"]) and pd.notna(row[\"longitude\"]) else None,\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Drop rows where geometry is None (invalid or missing lat/lon)\n",
        "col_df = col_df[col_df[\"geometry\"].notnull()]\n",
        "\n",
        "# Remove latitude and longitude columns now that geometry exists\n",
        "col_df = col_df.drop(columns=[\"latitude\", \"longitude\"])\n",
        "\n",
        "print(col_df.head())\n",
        "\n",
        "# Save updated dataframe\n",
        "col_df.to_csv(\"Cost_of_living_ranked_2.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaV9g_bBblHP",
        "outputId": "4467ec5e-c08c-4b83-b5fb-eb27ffcb8c0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  state      county    total_cost  cost_of_living_ranking  \\\n",
            "1    AR     Johnson  19146.670728                0.961861   \n",
            "2    CO  Las Animas  21664.262724                0.890135   \n",
            "3    CO       Routt  27986.883290                0.710005   \n",
            "5    GA       Rabun  19667.994026                0.947008   \n",
            "6    ID      Jerome  21749.163856                0.887716   \n",
            "\n",
            "                          geometry  \n",
            "1   POINT (-94.1654821 36.1328564)  \n",
            "2  POINT (-103.2224779 38.0664563)  \n",
            "3   POINT (-85.4666224 38.1342326)  \n",
            "5   POINT (-83.3864536 34.9572143)  \n",
            "6   POINT (-114.518808 42.7238458)  \n"
          ]
        }
      ]
    }
  ]
}